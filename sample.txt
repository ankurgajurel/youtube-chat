[{'text': 'humans need language to communicate so', 'start': 0.06, 'duration': 4.65}, {'text': 'it makes sense that AI does too', 'start': 2.7, 'duration': 6.84}, {'text': '[Music]', 'start': 4.71, 'duration': 7.83}, {'text': 'a large language model or llm is a type', 'start': 9.54, 'duration': 5.4}, {'text': 'of AI algorithm based on deep learning', 'start': 12.54, 'duration': 4.739}, {'text': 'and huge amounts of data that can', 'start': 14.94, 'duration': 4.5}, {'text': 'understand generate and predict new', 'start': 17.279, 'duration': 4.801}, {'text': "content language models aren't new the", 'start': 19.44, 'duration': 4.44}, {'text': 'first AI language model can be traced', 'start': 22.08, 'duration': 5.22}, {'text': 'back to 1966 but large language models', 'start': 23.88, 'duration': 5.7}, {'text': 'use a significantly larger pool of data', 'start': 27.3, 'duration': 3.96}, {'text': 'for training which means a significant', 'start': 29.58, 'duration': 3.78}, {'text': 'increase in the capabilities of the AI', 'start': 31.26, 'duration': 3.66}, {'text': 'model one of the most common', 'start': 33.36, 'duration': 3.84}, {'text': 'applications of llms right now is', 'start': 34.92, 'duration': 4.5}, {'text': 'generating content using AI chat Bots', 'start': 37.2, 'duration': 3.84}, {'text': 'more and more are popping up in the', 'start': 39.42, 'duration': 3.06}, {'text': 'market as competitors look to', 'start': 41.04, 'duration': 3.3}, {'text': 'differentiate themselves check out the', 'start': 42.48, 'duration': 3.3}, {'text': 'link above or in the description below', 'start': 44.34, 'duration': 3.6}, {'text': 'to see how two of The Front Runners chat', 'start': 45.78, 'duration': 5.16}, {'text': 'GPT and Bard compare and remember to', 'start': 47.94, 'duration': 5.22}, {'text': 'subscribe to ion tech for more videos on', 'start': 50.94, 'duration': 3.959}, {'text': 'all things business Tech', 'start': 53.16, 'duration': 4.26}, {'text': 'so just how large are large language', 'start': 54.899, 'duration': 5.221}, {'text': "models well there's no universally", 'start': 57.42, 'duration': 4.92}, {'text': 'accepted figure for how large an llm', 'start': 60.12, 'duration': 4.679}, {'text': "training data set is but it's typically", 'start': 62.34, 'duration': 5.34}, {'text': 'in the petabytes range for context a', 'start': 64.799, 'duration': 4.921}, {'text': 'single petabyte is equivalent to 1', 'start': 67.68, 'duration': 4.86}, {'text': 'million gigabytes the human brain is', 'start': 69.72, 'duration': 4.2}, {'text': 'believed to store about two and a half', 'start': 72.54, 'duration': 3.84}, {'text': 'petabytes of memory data', 'start': 73.92, 'duration': 4.739}, {'text': 'the llm training consists of multiple', 'start': 76.38, 'duration': 4.86}, {'text': 'steps usually starting with unsupervised', 'start': 78.659, 'duration': 4.021}, {'text': 'learning where the model starts to', 'start': 81.24, 'duration': 3.239}, {'text': 'derive relationships between words and', 'start': 82.68, 'duration': 4.799}, {'text': 'Concepts then fine-tuned with supervised', 'start': 84.479, 'duration': 5.161}, {'text': 'learning the training data then passes', 'start': 87.479, 'duration': 3.901}, {'text': 'through a Transformer which enables the', 'start': 89.64, 'duration': 3.72}, {'text': 'llm to recognize relationships and', 'start': 91.38, 'duration': 3.599}, {'text': 'connections using a self-attention', 'start': 93.36, 'duration': 2.82}, {'text': 'mechanism', 'start': 94.979, 'duration': 3.6}, {'text': 'once the llm is trained it can serve as', 'start': 96.18, 'duration': 4.759}, {'text': 'the base for any AI uses', 'start': 98.579, 'duration': 4.981}, {'text': 'llms can generate text translate', 'start': 100.939, 'duration': 5.381}, {'text': 'languages summarize or rewrite content', 'start': 103.56, 'duration': 5.64}, {'text': 'organize content analyze sentiment of', 'start': 106.32, 'duration': 5.7}, {'text': 'content like humor or tone and Converse', 'start': 109.2, 'duration': 4.8}, {'text': 'naturally with a user unlike older', 'start': 112.02, 'duration': 4.62}, {'text': 'generations of AI chatbot Technologies', 'start': 114.0, 'duration': 5.159}, {'text': 'llms can be particularly useful as a', 'start': 116.64, 'duration': 4.68}, {'text': 'foundation for customized uses for both', 'start': 119.159, 'duration': 4.981}, {'text': "businesses and individuals they're fast", 'start': 121.32, 'duration': 5.28}, {'text': 'accurate flexible and easy to train', 'start': 124.14, 'duration': 5.06}, {'text': 'however users should heed caution too', 'start': 126.6, 'duration': 4.859}, {'text': 'llms come with a number of challenges', 'start': 129.2, 'duration': 4.42}, {'text': 'like the cost of deployment and', 'start': 131.459, 'duration': 4.981}, {'text': 'operation bias depending on what data it', 'start': 133.62, 'duration': 5.94}, {'text': 'was trained on AI hallucinations where a', 'start': 136.44, 'duration': 4.62}, {'text': 'response is not based off of the', 'start': 139.56, 'duration': 3.72}, {'text': 'training data troubleshooting complexity', 'start': 141.06, 'duration': 5.22}, {'text': 'in glitch tokens or words or inputs', 'start': 143.28, 'duration': 5.16}, {'text': 'maliciously designed to make the llm', 'start': 146.28, 'duration': 4.98}, {'text': 'malfunction how have you used llms what', 'start': 148.44, 'duration': 4.14}, {'text': 'benefits or challenges have you', 'start': 151.26, 'duration': 3.18}, {'text': 'experienced share your thoughts in the', 'start': 152.58, 'duration': 3.239}, {'text': 'comments and remember to like And', 'start': 154.44, 'duration': 2.4}, {'text': 'subscribe', 'start': 155.819, 'duration': 3.42}, {'text': '[Music]', 'start': 156.84, 'duration': 5.399}, {'text': 'foreign', 'start': 159.239, 'duration': 3.0}]
[{'text': 'foreign', 'start': 0.24, 'duration': 3.14}, {'text': '[Music]', 'start': 0.88, 'duration': 5.72}, {'text': 'has a curious parrot called buddy buddy', 'start': 3.38, 'duration': 5.139}, {'text': 'has a great mimicking ability and a', 'start': 6.6, 'duration': 3.3}, {'text': 'sharp memory', 'start': 8.519, 'duration': 3.361}, {'text': 'buddy listens to all the conversations', 'start': 9.9, 'duration': 4.5}, {'text': "in Peter's home and can mimic them very", 'start': 11.88, 'duration': 4.739}, {'text': 'accurately now when he hears feeling', 'start': 14.4, 'duration': 4.98}, {'text': 'hungry I would like to have some', 'start': 16.619, 'duration': 4.981}, {'text': 'for this case the probability of him', 'start': 19.38, 'duration': 4.319}, {'text': 'saying Biryani cherries or food is much', 'start': 21.6, 'duration': 4.679}, {'text': 'higher than the words such as bicycle or', 'start': 23.699, 'duration': 3.361}, {'text': 'book', 'start': 26.279, 'duration': 2.701}, {'text': "but he doesn't understand the meaning of", 'start': 27.06, 'duration': 4.139}, {'text': 'Biryani or food or cherries the way', 'start': 28.98, 'duration': 4.919}, {'text': 'humans do all he is doing is using', 'start': 31.199, 'duration': 5.161}, {'text': 'statistical probability along with some', 'start': 33.899, 'duration': 4.801}, {'text': 'Randomness to predict the next word or', 'start': 36.36, 'duration': 4.68}, {'text': 'set of words be only based on the past', 'start': 38.7, 'duration': 5.16}, {'text': 'conversations he has listened to we can', 'start': 41.04, 'duration': 5.1}, {'text': 'call Buddy a stochastic parrot', 'start': 43.86, 'duration': 4.56}, {'text': 'stochastic Means A system that is', 'start': 46.14, 'duration': 4.14}, {'text': 'characterized by Randomness or', 'start': 48.42, 'duration': 3.299}, {'text': 'probability', 'start': 50.28, 'duration': 3.48}, {'text': 'a language model is somewhat like a', 'start': 51.719, 'duration': 3.781}, {'text': 'stochastic parrot their computer', 'start': 53.76, 'duration': 3.779}, {'text': 'programs that use a technology called', 'start': 55.5, 'duration': 4.44}, {'text': 'neural networks to predict the next set', 'start': 57.539, 'duration': 4.981}, {'text': 'of words for a sentence for a simple', 'start': 59.94, 'duration': 4.38}, {'text': 'explanation of a neural network please', 'start': 62.52, 'duration': 3.779}, {'text': 'watch this particular video', 'start': 64.32, 'duration': 4.74}, {'text': "just like how birdies strain on Peter's", 'start': 66.299, 'duration': 5.341}, {'text': 'home conversations data set you can have', 'start': 69.06, 'duration': 4.919}, {'text': 'a language model that is trained on for', 'start': 71.64, 'duration': 5.159}, {'text': 'example all movie related articles from', 'start': 73.979, 'duration': 5.101}, {'text': 'Wikipedia and it will be able to predict', 'start': 76.799, 'duration': 3.841}, {'text': 'the next set of words for a movie', 'start': 79.08, 'duration': 4.5}, {'text': 'related sentence Gmail autocomplete is', 'start': 80.64, 'duration': 5.46}, {'text': 'one of the many applications that uses a', 'start': 83.58, 'duration': 4.32}, {'text': 'language model underneath', 'start': 86.1, 'duration': 3.839}, {'text': 'now that we have some understanding of a', 'start': 87.9, 'duration': 4.02}, {'text': "language model let's understand what the", 'start': 89.939, 'duration': 4.201}, {'text': 'heck is a large language model', 'start': 91.92, 'duration': 4.559}, {'text': "let's go back to our buddy example our", 'start': 94.14, 'duration': 5.7}, {'text': 'buddy got some Divine super power and', 'start': 96.479, 'duration': 6.361}, {'text': "now he can listen to Peter's neighbors", 'start': 99.84, 'duration': 5.16}, {'text': 'conversations conversations that are', 'start': 102.84, 'duration': 4.68}, {'text': 'happening in schools and universities in', 'start': 105.0, 'duration': 5.399}, {'text': 'the town in fact not only in his town', 'start': 107.52, 'duration': 6.18}, {'text': 'but all the towns across the world', 'start': 110.399, 'duration': 5.821}, {'text': 'with this extra power and knowledge now', 'start': 113.7, 'duration': 4.919}, {'text': 'buddy can complete the next set of words', 'start': 116.22, 'duration': 4.2}, {'text': 'on a history subject', 'start': 118.619, 'duration': 4.561}, {'text': 'give your nutrition advice or even write', 'start': 120.42, 'duration': 6.0}, {'text': 'a poem like our powerful parrot body', 'start': 123.18, 'duration': 5.999}, {'text': 'large language models are trained on a', 'start': 126.42, 'duration': 5.039}, {'text': 'huge volume of data such as Wikipedia', 'start': 129.179, 'duration': 4.861}, {'text': 'articles Google news articles online', 'start': 131.459, 'duration': 4.261}, {'text': 'books and so on', 'start': 134.04, 'duration': 4.62}, {'text': 'if you look inside the llm you will find', 'start': 135.72, 'duration': 5.04}, {'text': 'a neural network containing trillions of', 'start': 138.66, 'duration': 4.38}, {'text': 'parameters that can capture more complex', 'start': 140.76, 'duration': 5.1}, {'text': 'patterns and nuances in a language chat', 'start': 143.04, 'duration': 5.16}, {'text': 'GPT is an application that uses llm', 'start': 145.86, 'duration': 6.9}, {'text': 'called gpt3 or gpt4 behind the scenes', 'start': 148.2, 'duration': 7.38}, {'text': 'examples of llms are Palm 2 by Google', 'start': 152.76, 'duration': 5.76}, {'text': 'and llama by meta', 'start': 155.58, 'duration': 5.939}, {'text': 'on top of statistical predictions llm', 'start': 158.52, 'duration': 4.939}, {'text': 'uses another approach called', 'start': 161.519, 'duration': 4.141}, {'text': 'reinforcement learning with human', 'start': 163.459, 'duration': 6.041}, {'text': "feedback rlhf let's understand this once", 'start': 165.66, 'duration': 6.299}, {'text': 'again with Buddy one day Peter was', 'start': 169.5, 'duration': 4.319}, {'text': 'having a conversation with his cute', 'start': 171.959, 'duration': 4.701}, {'text': 'little two-year-old son', 'start': 173.819, 'duration': 7.881}, {'text': "don't eat too much bananas else", 'start': 176.66, 'duration': 5.04}, {'text': 'hearing this Peter realized that buddy', 'start': 183.3, 'duration': 4.439}, {'text': 'has been listening to the conversations', 'start': 185.879, 'duration': 5.761}, {'text': 'from abusive parents in his town what he', 'start': 187.739, 'duration': 6.061}, {'text': 'said was the effect of that', 'start': 191.64, 'duration': 4.319}, {'text': 'Peter then starts skipping a close eye', 'start': 193.8, 'duration': 4.5}, {'text': 'on what buddy is saying for a same', 'start': 195.959, 'duration': 4.2}, {'text': 'question buddy can produce multiple', 'start': 198.3, 'duration': 5.159}, {'text': 'answers and all Peter has to do is tell', 'start': 200.159, 'duration': 5.821}, {'text': 'him which one is toxic and which one is', 'start': 203.459, 'duration': 3.541}, {'text': 'not', 'start': 205.98, 'duration': 3.119}, {'text': "after this training buddy doesn't use", 'start': 207.0, 'duration': 4.019}, {'text': 'any toxic language', 'start': 209.099, 'duration': 4.681}, {'text': 'while training chat GPT open air used a', 'start': 211.019, 'duration': 5.22}, {'text': 'similar approach of human intervention', 'start': 213.78, 'duration': 4.44}, {'text': 'rlhf', 'start': 216.239, 'duration': 5.22}, {'text': 'open air used a huge Workforce of humans', 'start': 218.22, 'duration': 6.599}, {'text': 'to make chat GPT less toxic while llms', 'start': 221.459, 'duration': 5.161}, {'text': "are very powerful they don't have any", 'start': 224.819, 'duration': 4.381}, {'text': 'subjective experience emotions or', 'start': 226.62, 'duration': 5.179}, {'text': 'Consciousness that we as humans have', 'start': 229.2, 'duration': 5.28}, {'text': 'llms work purely based on the data that', 'start': 231.799, 'duration': 4.241}, {'text': 'they have been trained on I hope you', 'start': 234.48, 'duration': 3.66}, {'text': 'like this short explanation which was', 'start': 236.04, 'duration': 4.559}, {'text': 'based on analogy obviously the technical', 'start': 238.14, 'duration': 4.98}, {'text': 'working of this thing is little', 'start': 240.599, 'duration': 4.261}, {'text': 'different than analogy but this should', 'start': 243.12, 'duration': 4.08}, {'text': 'give you a good intuition on this topic', 'start': 244.86, 'duration': 4.26}, {'text': 'if you like this video please share with', 'start': 247.2, 'duration': 5.899}, {'text': 'those who are curious about this topic', 'start': 249.12, 'duration': 3.979}, {'text': 'foreign', 'start': 255.18, 'duration': 3.0}]
[{'text': "SPEAKER 1: Unless you've\nbeen living under a rock,", 'start': 0.0, 'duration': 1.84}, {'text': "you've probably heard\nthat AI is getting", 'start': 1.84, 'duration': 1.667}, {'text': 'very good at conversation.', 'start': 3.507, 'duration': 1.426}, {'text': 'In fact, maybe you\neven chatted with one', 'start': 4.933, 'duration': 1.667}, {'text': "of these AI's through a chat\nbot interface like Google Bard.", 'start': 6.6, 'duration': 3.0}, {'text': 'SPEAKER 2: This is all\nthanks to a powerful kind', 'start': 9.6, 'duration': 2.01}, {'text': 'of neural network called a\nLarge Language Model, or LLM.', 'start': 11.61, 'duration': 4.13}, {'text': 'LLMs enable computers to\nunderstand and generate', 'start': 15.74, 'duration': 2.95}, {'text': 'language better\nthan ever before,', 'start': 18.69, 'duration': 1.8}, {'text': 'unlocking a whole host\nof new applications.', 'start': 20.49, 'duration': 2.508}, {'text': "SPEAKER 1: In this\nvideo, we're going", 'start': 22.998, 'duration': 1.542}, {'text': 'to talk about what LLMs\nare and how anyone can get', 'start': 24.54, 'duration': 2.083}, {'text': "started building with them,\nwhether you're a developer", 'start': 26.623, 'duration': 2.25}, {'text': 'or not.', 'start': 28.873, 'duration': 0.587}, {'text': 'SPEAKER 2: Ready?', 'start': 29.46, 'duration': 0.84}, {'text': "BOTH: Let's dive in.", 'start': 30.3, 'duration': 2.668}, {'text': 'SPEAKER 1: LLMs are\nmachine learning models', 'start': 32.968, 'duration': 1.792}, {'text': 'that are really good at\nunderstanding and generating', 'start': 34.76, 'duration': 2.167}, {'text': 'human language.', 'start': 36.927, 'duration': 0.683}, {'text': "They're based on transformers,\na type of neural network", 'start': 37.61, 'duration': 2.34}, {'text': 'architecture invented by Google.', 'start': 39.95, 'duration': 1.77}, {'text': 'Now, what made the transformer\narchitecture so powerful', 'start': 41.72, 'duration': 2.85}, {'text': 'was its ability to\nscale effectively,', 'start': 44.57, 'duration': 1.86}, {'text': 'allowing us to train these\nmodels on massive text', 'start': 46.43, 'duration': 2.13}, {'text': 'datasets.', 'start': 48.56, 'duration': 0.72}, {'text': 'SPEAKER 2: That\'s where the\n"large" in large language', 'start': 49.28, 'duration': 2.208}, {'text': 'models comes from--', 'start': 51.488, 'duration': 0.832}, {'text': 'both the size and complexity\nof the neural network itself,', 'start': 52.32, 'duration': 2.84}, {'text': 'as well as the size of the\ndataset that it was trained on.', 'start': 55.16, 'duration': 3.0}, {'text': "For some of these\nmodels, we're talking", 'start': 58.16, 'duration': 1.68}, {'text': 'about trillions of\ntokens from a bunch', 'start': 59.84, 'duration': 2.04}, {'text': 'of publicly available sources.', 'start': 61.88, 'duration': 2.189}, {'text': "And it wasn't until\nresearchers started", 'start': 64.069, 'duration': 1.681}, {'text': 'to make these\nmodels really large', 'start': 65.75, 'duration': 1.71}, {'text': 'and train them on\nthese huge datasets', 'start': 67.46, 'duration': 1.92}, {'text': 'that they started showing\nthese impressive results,', 'start': 69.38, 'duration': 2.4}, {'text': 'like understanding complex,\nnuanced language and generating', 'start': 71.78, 'duration': 3.72}, {'text': 'language more\neloquently than ever.', 'start': 75.5, 'duration': 1.89}, {'text': "SPEAKER 1: If you're already\nfamiliar with machine learning,", 'start': 77.39, 'duration': 2.5}, {'text': 'you probably think\nabout training', 'start': 79.89, 'duration': 1.375}, {'text': 'a model for a\nspecific task, like is', 'start': 81.265, 'duration': 2.395}, {'text': 'this tweet positive or\nnegative, or translate this text', 'start': 83.66, 'duration': 2.79}, {'text': 'from French to English.', 'start': 86.45, 'duration': 2.11}, {'text': 'What makes LLMs\nespecially powerful', 'start': 88.56, 'duration': 2.12}, {'text': 'is that one model can be used\nfor a whole variety of tasks,', 'start': 90.68, 'duration': 2.73}, {'text': 'like chat, copywriting,\ntranslation, summarization,', 'start': 93.41, 'duration': 3.57}, {'text': 'brainstorming, code generation,\nand a whole lot more.', 'start': 96.98, 'duration': 2.805}, {'text': 'SPEAKER 2: Best of all, you can\nprototype language applications', 'start': 99.785, 'duration': 2.625}, {'text': 'incredibly fast with LLMs--', 'start': 102.41, 'duration': 1.98}, {'text': 'in just minutes,\nrather than months.', 'start': 104.39, 'duration': 1.757}, {'text': "And you don't have to be\na machine learning expert", 'start': 106.147, 'duration': 2.083}, {'text': 'to do it.', 'start': 108.23, 'duration': 0.9}, {'text': 'All you really need to\nknow is how to write.', 'start': 109.13, 'duration': 2.55}, {'text': 'So how do you\nactually use an LLM?', 'start': 111.68, 'duration': 2.01}, {'text': "Well, let's take a look.", 'start': 113.69, 'duration': 1.85}, {'text': 'LLMs learn about\npatterns and language', 'start': 115.54, 'duration': 2.17}, {'text': "from the massive amounts of\ntext data they're trained on.", 'start': 117.71, 'duration': 2.7}, {'text': 'Then they take as\ninput some text', 'start': 120.41, 'duration': 2.28}, {'text': "and produce some output text\nthat's likely to follow.", 'start': 122.69, 'duration': 2.633}, {'text': 'SPEAKER 1: Another\nway to say this', 'start': 125.323, 'duration': 1.417}, {'text': 'is that LLMs are like really\nsophisticated autocomplete.', 'start': 126.74, 'duration': 2.94}, {'text': 'So for example, if we\ngive an LLM the input--', 'start': 129.68, 'duration': 2.61}, {'text': "SPEAKER 2: It's\nraining cats and--", 'start': 132.29, 'duration': 1.48}, {'text': 'SPEAKER 1: It\'ll probably\npredict that "dogs" is', 'start': 133.77, 'duration': 2.0}, {'text': 'the most likely word to follow.', 'start': 135.77, 'duration': 1.68}, {'text': 'Now, this might not\nseem that exciting,', 'start': 137.45, 'duration': 1.8}, {'text': 'but we can actually use this\nautocomplete-like functionality', 'start': 139.25, 'duration': 2.88}, {'text': 'to solve tons of tasks just by\nwriting strategic text input.', 'start': 142.13, 'duration': 3.51}, {'text': "SPEAKER 2: For example,\nlet's take Google's PaLM", 'start': 145.64, 'duration': 2.37}, {'text': 'LLM and input this sentence.', 'start': 148.01, 'duration': 2.048}, {'text': 'SPEAKER 1: I have two\napples and I eat one.', 'start': 150.058, 'duration': 1.792}, {'text': "I'm left with--", 'start': 151.85, 'duration': 1.14}, {'text': 'SPEAKER 2: The PaLM model\noutputs the answer "one."', 'start': 152.99, 'duration': 2.34}, {'text': 'In this way, we get the LLM\nto perform some simple math.', 'start': 155.33, 'duration': 3.282}, {'text': 'SPEAKER 1: Or take\nanother example.', 'start': 158.612, 'duration': 1.458}, {'text': 'SPEAKER 2: Paris is to\nFrance as Tokyo is to--', 'start': 160.07, 'duration': 3.87}, {'text': 'SPEAKER 1: The PaLM model\noutputs "Japan," which tells us', 'start': 163.94, 'duration': 2.4}, {'text': 'that the model can not\nonly complete analogies,', 'start': 166.34, 'duration': 2.013}, {'text': "but it also has some\nworld knowledge that it's", 'start': 168.353, 'duration': 1.917}, {'text': 'learned from its training data.', 'start': 170.27, 'duration': 2.07}, {'text': 'So I should add the\ncaveat that not all', 'start': 172.34, 'duration': 1.68}, {'text': 'of the knowledge\nthat the LLM outputs', 'start': 174.02, 'duration': 1.62}, {'text': 'is necessarily\nfactually accurate.', 'start': 175.64, 'duration': 2.16}, {'text': 'SPEAKER 2: Now, all of the\ntext that we feed into an LLM', 'start': 177.8, 'duration': 2.64}, {'text': 'as input is called a\nprompt, and it turns out', 'start': 180.44, 'duration': 2.76}, {'text': "there's this whole art known\nas prompt design, which", 'start': 183.2, 'duration': 2.58}, {'text': 'is about figuring out\nhow to write and format', 'start': 185.78, 'duration': 2.1}, {'text': 'prompt text to get LLMs\nto do what you want.', 'start': 187.88, 'duration': 2.937}, {'text': 'SPEAKER 1: For example,\none way to structure', 'start': 190.817, 'duration': 1.833}, {'text': 'a prompt is as an\ninstruction, like--', 'start': 192.65, 'duration': 2.1}, {'text': 'SPEAKER 2: Write me a\npoem about Ada Lovelace', 'start': 194.75, 'duration': 2.1}, {'text': 'in the style of Shakespeare.', 'start': 196.85, 'duration': 2.28}, {'text': "SPEAKER 1: Or explain quantum\nphysics to me like I'm five.", 'start': 199.13, 'duration': 3.157}, {'text': 'SPEAKER 2: Or generate\na list of items', 'start': 202.287, 'duration': 1.583}, {'text': 'I need for a camping trip\nto Yosemite National Park.', 'start': 203.87, 'duration': 3.18}, {'text': 'SPEAKER 1: This approach--\nusing a single command', 'start': 207.05, 'duration': 2.07}, {'text': 'to get an LLM to\ntake on a behavior--', 'start': 209.12, 'duration': 1.65}, {'text': 'is called zero shot learning.', 'start': 210.77, 'duration': 1.98}, {'text': 'But in addition to just\nproviding an instruction,', 'start': 212.75, 'duration': 2.19}, {'text': 'it can be helpful to\nshow the model what', 'start': 214.94, 'duration': 1.667}, {'text': 'you want by adding examples.', 'start': 216.607, 'duration': 1.433}, {'text': 'This is called few shot\nlearning because we showed', 'start': 218.04, 'duration': 2.083}, {'text': 'the model a few examples.', 'start': 220.123, 'duration': 1.237}, {'text': "Like here's a prompt for\ntranslating from English", 'start': 221.36, 'duration': 2.13}, {'text': 'to French.', 'start': 223.49, 'duration': 1.14}, {'text': 'First we provide an instruction.', 'start': 224.63, 'duration': 2.37}, {'text': 'Then we give some examples,\nestablishing the text pattern.', 'start': 227.0, 'duration': 4.5}, {'text': 'If we pass this prompt\nto an LLM like PaLM,', 'start': 231.5, 'duration': 2.22}, {'text': 'we get back something\nlike the following.', 'start': 233.72, 'duration': 2.52}, {'text': 'SPEAKER 2: The model did\nprovide a French translation', 'start': 236.24, 'duration': 2.25}, {'text': 'of lipstick, but you might\nnotice that it went on', 'start': 238.49, 'duration': 2.4}, {'text': 'to generate all these additional\nEnglish-French translation', 'start': 240.89, 'duration': 2.79}, {'text': 'pairs.', 'start': 243.68, 'duration': 0.72}, {'text': 'This might seem a\nlittle unexpected,', 'start': 244.4, 'duration': 1.83}, {'text': 'but the LLM is just\ncompleting the pattern', 'start': 246.23, 'duration': 2.19}, {'text': 'that we gave it in the prompt.', 'start': 248.42, 'duration': 2.04}, {'text': "As another example,\nhere's a few shot", 'start': 250.46, 'duration': 1.92}, {'text': 'prompt to convert Python\ncode snippets to JavaScript.', 'start': 252.38, 'duration': 3.21}, {'text': 'Our prompt starts\nwith an instruction,', 'start': 255.59, 'duration': 2.61}, {'text': 'then we have some examples,\nand finally, the Python code', 'start': 258.2, 'duration': 3.509}, {'text': 'we actually want converted.', 'start': 261.709, 'duration': 2.221}, {'text': 'The very last part\nof this prompt', 'start': 263.93, 'duration': 1.53}, {'text': 'is JavaScript colon\nbecause we want', 'start': 265.46, 'duration': 2.25}, {'text': 'to nudge the model to output\nsome JavaScript code just', 'start': 267.71, 'duration': 3.24}, {'text': 'like this.', 'start': 270.95, 'duration': 1.028}, {'text': 'SPEAKER 1: Note that\nin a real application,', 'start': 271.978, 'duration': 1.792}, {'text': 'we probably want to parameterize\nthe input instead of hard', 'start': 273.77, 'duration': 2.43}, {'text': 'coding it into the prompt.', 'start': 276.2, 'duration': 1.083}, {'text': 'That way, our users can\nprovide the Python code', 'start': 277.283, 'duration': 2.157}, {'text': 'that they want converted.', 'start': 279.44, 'duration': 1.77}, {'text': 'And this is essentially how\nyou would customize an LLM', 'start': 281.21, 'duration': 2.4}, {'text': 'for a Python to JavaScript app.', 'start': 283.61, 'duration': 2.797}, {'text': 'SPEAKER 2: Now, you\nmight be wondering', 'start': 286.407, 'duration': 1.583}, {'text': 'what the absolute best way\nto write a model prompt is.', 'start': 287.99, 'duration': 2.82}, {'text': "And if so, we've got\nsome bad news for you.", 'start': 290.81, 'duration': 2.738}, {'text': "SPEAKER 1: There's\ncurrently no optimal way", 'start': 293.548, 'duration': 1.792}, {'text': "to write model\nprompts, and that's", 'start': 295.34, 'duration': 1.26}, {'text': 'because the results we get\nare so highly dependent', 'start': 296.6, 'duration': 2.13}, {'text': 'on the underlying model.', 'start': 298.73, 'duration': 1.38}, {'text': 'Sometimes small changes in\nwording or even in word order', 'start': 300.11, 'duration': 2.85}, {'text': "can improve the LLM's outputs\nin ways that are not always", 'start': 302.96, 'duration': 2.7}, {'text': 'predictable.', 'start': 305.66, 'duration': 0.5}, {'text': "SPEAKER 2: That's\nwhy it's always worth", 'start': 306.16, 'duration': 1.625}, {'text': 'trying out lots of different\nstructures and examples', 'start': 307.785, 'duration': 2.195}, {'text': 'and formats and seeing what\nworks best for your use case.', 'start': 309.98, 'duration': 3.062}, {'text': 'SPEAKER 1: There you have it.', 'start': 313.042, 'duration': 1.208}, {'text': "That's the magic of\nLLMs in a nutshell.", 'start': 314.25, 'duration': 1.73}, {'text': 'SPEAKER 2: You can check\nout Bard at bard.google.com,', 'start': 315.98, 'duration': 2.64}, {'text': 'and definitely let us\nknow in the comments', 'start': 318.62, 'duration': 1.86}, {'text': "below what you're\nbuilding with LLMs.", 'start': 320.48, 'duration': 1.62}, {'text': '[MUSIC PLAYING]', 'start': 322.1, 'duration': 3.05}]
[{'text': 'hello everyone and welcome to the video', 'start': 0.0, 'duration': 4.14}, {'text': 'on large language models large language', 'start': 2.04, 'duration': 4.02}, {'text': 'models are basically very Advanced', 'start': 4.14, 'duration': 3.959}, {'text': 'artificial intelligence systems that can', 'start': 6.06, 'duration': 3.9}, {'text': 'process and generate massive amounts of', 'start': 8.099, 'duration': 3.721}, {'text': 'text Data they are designed to learn', 'start': 9.96, 'duration': 3.78}, {'text': 'from and understand natural human', 'start': 11.82, 'duration': 3.899}, {'text': 'language and can be used to perform a', 'start': 13.74, 'duration': 3.84}, {'text': 'wide range of language related tasks', 'start': 15.719, 'duration': 4.14}, {'text': 'such as translation speech recognition', 'start': 17.58, 'duration': 4.8}, {'text': 'and automatic summary generation one of', 'start': 19.859, 'duration': 4.021}, {'text': 'the key advantages of large language', 'start': 22.38, 'duration': 3.6}, {'text': 'models is their ability to learn from', 'start': 23.88, 'duration': 4.139}, {'text': 'vast amounts of data which allows them', 'start': 25.98, 'duration': 3.719}, {'text': 'to generate highly accurate and', 'start': 28.019, 'duration': 3.961}, {'text': 'realistic responses to complex natural', 'start': 29.699, 'duration': 4.501}, {'text': 'language prompts additionally they can', 'start': 31.98, 'duration': 3.54}, {'text': 'be trained on multiple languages', 'start': 34.2, 'duration': 3.66}, {'text': 'simultaneously which means they can be', 'start': 35.52, 'duration': 3.84}, {'text': 'used to perform language translation', 'start': 37.86, 'duration': 3.3}, {'text': 'between different pairs of languages', 'start': 39.36, 'duration': 3.9}, {'text': 'making them an invaluable tool for', 'start': 41.16, 'duration': 4.44}, {'text': 'businesses and organizations that work', 'start': 43.26, 'duration': 4.319}, {'text': 'with people from diverse cultural', 'start': 45.6, 'duration': 3.9}, {'text': 'backgrounds not just language related', 'start': 47.579, 'duration': 4.261}, {'text': 'tasks but llms can also have the', 'start': 49.5, 'duration': 4.26}, {'text': 'potential to revolutionize Fields such', 'start': 51.84, 'duration': 4.44}, {'text': 'as research science and Healthcare by', 'start': 53.76, 'duration': 4.38}, {'text': 'allowing researchers to quickly analyze', 'start': 56.28, 'duration': 4.02}, {'text': 'and process vast amounts of complex X', 'start': 58.14, 'duration': 4.14}, {'text': 'data they can accelerate progress in', 'start': 60.3, 'duration': 4.319}, {'text': 'areas such as drug Discovery Healthcare', 'start': 62.28, 'duration': 4.56}, {'text': 'Diagnostics and AI development that', 'start': 64.619, 'duration': 4.441}, {'text': 'being said large language models also', 'start': 66.84, 'duration': 4.02}, {'text': 'present some challenges for instance', 'start': 69.06, 'duration': 3.78}, {'text': 'they require a significant amount of', 'start': 70.86, 'duration': 3.84}, {'text': 'compute resources which can be', 'start': 72.84, 'duration': 3.48}, {'text': 'prohibitively expensive for some', 'start': 74.7, 'duration': 3.9}, {'text': 'organizations additionally because they', 'start': 76.32, 'duration': 4.26}, {'text': 'are trained on huge amounts of data they', 'start': 78.6, 'duration': 4.559}, {'text': 'may inadvertently reinforce biased or', 'start': 80.58, 'duration': 4.26}, {'text': 'discriminate the patterns in language', 'start': 83.159, 'duration': 3.901}, {'text': 'use which is something that researchers', 'start': 84.84, 'duration': 4.139}, {'text': 'and Engineers are continuously working', 'start': 87.06, 'duration': 4.68}, {'text': 'to address overall large language models', 'start': 88.979, 'duration': 4.68}, {'text': 'are an exciting and rapidly developing', 'start': 91.74, 'duration': 3.72}, {'text': 'field with tremendous potential to', 'start': 93.659, 'duration': 4.261}, {'text': 'transform the way we live work and', 'start': 95.46, 'duration': 4.56}, {'text': 'communicate so if you want to embark on', 'start': 97.92, 'duration': 4.44}, {'text': 'a journey of AI and ml then try giving a', 'start': 100.02, 'duration': 4.38}, {'text': 'show to a postgraduate program in Ai and', 'start': 102.36, 'duration': 4.259}, {'text': 'ml that is in partnership with IBM this', 'start': 104.4, 'duration': 3.96}, {'text': 'artificial intelligence course covers', 'start': 106.619, 'duration': 3.421}, {'text': 'the latest tools and Technologies from', 'start': 108.36, 'duration': 3.84}, {'text': 'the AI ecosystem and features master', 'start': 110.04, 'duration': 4.259}, {'text': 'classes by Caltech faculty and IBM', 'start': 112.2, 'duration': 4.02}, {'text': 'experts hackathons and ask me anything', 'start': 114.299, 'duration': 4.5}, {'text': 'sessions this program showcases Caltech', 'start': 116.22, 'duration': 4.8}, {'text': "ctm is excellence and I IBM's industry", 'start': 118.799, 'duration': 4.261}, {'text': 'progress the artificial intelligence', 'start': 121.02, 'duration': 3.959}, {'text': 'course covers key Concepts like', 'start': 123.06, 'duration': 3.839}, {'text': 'statistics data science with python', 'start': 124.979, 'duration': 4.381}, {'text': 'machine learning deep learning NLP and', 'start': 126.899, 'duration': 4.321}, {'text': 'reinforcement learning through an', 'start': 129.36, 'duration': 3.36}, {'text': 'Interactive Learning model with live', 'start': 131.22, 'duration': 4.379}, {'text': 'sessions enroll now analog exciting Ai', 'start': 132.72, 'duration': 4.68}, {'text': 'and ml opportunities the link is', 'start': 135.599, 'duration': 3.301}, {'text': 'mentioned in the description box below', 'start': 137.4, 'duration': 4.02}, {'text': 'and with that having said hey everyone', 'start': 138.9, 'duration': 4.44}, {'text': 'welcome to Simply learns YouTube channel', 'start': 141.42, 'duration': 4.26}, {'text': "but before we dive into that don't", 'start': 143.34, 'duration': 5.039}, {'text': 'forget to like subscribe and share in', 'start': 145.68, 'duration': 4.62}, {'text': "this video we'll cover topics like what", 'start': 148.379, 'duration': 3.901}, {'text': 'are large language models after that', 'start': 150.3, 'duration': 4.38}, {'text': "we'll look at what large language models", 'start': 152.28, 'duration': 5.16}, {'text': "used for and after that we'll cover how", 'start': 154.68, 'duration': 4.74}, {'text': 'are large language models trained after', 'start': 157.44, 'duration': 4.379}, {'text': "this we'll look at how do large language", 'start': 159.42, 'duration': 4.86}, {'text': "models work and at the last we'll see", 'start': 161.819, 'duration': 4.5}, {'text': 'some applications of large language', 'start': 164.28, 'duration': 5.22}, {'text': "models so without any further Ado let's", 'start': 166.319, 'duration': 5.821}, {'text': 'get started', 'start': 169.5, 'duration': 4.62}, {'text': "so we'll start with what are the large", 'start': 172.14, 'duration': 4.08}, {'text': 'language models large language models', 'start': 174.12, 'duration': 4.979}, {'text': 'such as gpd3 generative pre-trained', 'start': 176.22, 'duration': 5.04}, {'text': 'Transformer 3 Advanced artificial', 'start': 179.099, 'duration': 4.081}, {'text': 'intelligence systems designed to', 'start': 181.26, 'duration': 4.559}, {'text': 'understand and generate human-like text', 'start': 183.18, 'duration': 4.8}, {'text': 'these models are built using deep', 'start': 185.819, 'duration': 3.841}, {'text': 'learning techniques and have been', 'start': 187.98, 'duration': 3.72}, {'text': 'trained on vast amounts of text Data', 'start': 189.66, 'duration': 4.56}, {'text': 'from the internet these models use', 'start': 191.7, 'duration': 5.16}, {'text': 'self-attention mechanisms to analyze the', 'start': 194.22, 'duration': 5.28}, {'text': 'relationships between words or tokens in', 'start': 196.86, 'duration': 4.379}, {'text': 'a text enabling them to capture', 'start': 199.5, 'duration': 4.379}, {'text': 'contextual information and generate core', 'start': 201.239, 'duration': 4.86}, {'text': 'entry responses these models use', 'start': 203.879, 'duration': 4.381}, {'text': 'self-attention mechanisms to analyze the', 'start': 206.099, 'duration': 4.381}, {'text': 'relationship between different words or', 'start': 208.26, 'duration': 4.619}, {'text': 'tokens in the text enabling them to', 'start': 210.48, 'duration': 4.32}, {'text': 'capture contextual information and', 'start': 212.879, 'duration': 4.561}, {'text': 'generate core and responses these models', 'start': 214.8, 'duration': 4.799}, {'text': 'have significant implications for a wide', 'start': 217.44, 'duration': 4.92}, {'text': 'range of applications including virtual', 'start': 219.599, 'duration': 5.101}, {'text': 'assistants chat boards content', 'start': 222.36, 'duration': 3.36}, {'text': 'generation', 'start': 224.7, 'duration': 3.42}, {'text': 'language translation and aiding in', 'start': 225.72, 'duration': 4.439}, {'text': 'research and decision making processes', 'start': 228.12, 'duration': 4.44}, {'text': 'their ability to generate coherent and', 'start': 230.159, 'duration': 4.8}, {'text': 'contextually appropriate text has led to', 'start': 232.56, 'duration': 3.899}, {'text': 'advancement in natural language', 'start': 234.959, 'duration': 3.301}, {'text': 'understanding and human computer', 'start': 236.459, 'duration': 3.0}, {'text': 'interaction', 'start': 238.26, 'duration': 3.96}, {'text': "now we'll see what are language models", 'start': 239.459, 'duration': 4.14}, {'text': 'used for', 'start': 242.22, 'duration': 3.96}, {'text': 'so large language models are utilized in', 'start': 243.599, 'duration': 4.801}, {'text': 'scenarios where there is limited or no', 'start': 246.18, 'duration': 4.139}, {'text': 'domain specific data available for', 'start': 248.4, 'duration': 3.899}, {'text': 'training these scenarios include both', 'start': 250.319, 'duration': 3.901}, {'text': 'few short and zero short learning', 'start': 252.299, 'duration': 4.261}, {'text': "approaches which rely on the model's", 'start': 254.22, 'duration': 4.5}, {'text': 'strong inductive bias and its capability', 'start': 256.56, 'duration': 4.079}, {'text': 'to derive meaningful representations', 'start': 258.72, 'duration': 4.44}, {'text': 'from a small amount of data or even no', 'start': 260.639, 'duration': 3.721}, {'text': 'data at all', 'start': 263.16, 'duration': 3.3}, {'text': "now we'll see how our large language", 'start': 264.36, 'duration': 4.44}, {'text': 'models trained large language models', 'start': 266.46, 'duration': 4.56}, {'text': 'typically undergo pre-training on abroad', 'start': 268.8, 'duration': 4.679}, {'text': 'while encompassing data set that shares', 'start': 271.02, 'duration': 4.56}, {'text': 'statistical similarities with the data', 'start': 273.479, 'duration': 4.44}, {'text': 'set specific to the Target task the', 'start': 275.58, 'duration': 4.08}, {'text': 'objective of pre-training is to enable', 'start': 277.919, 'duration': 3.661}, {'text': 'the model to acquire high level features', 'start': 279.66, 'duration': 3.84}, {'text': 'that can later be applied during the', 'start': 281.58, 'duration': 5.04}, {'text': 'fine tuning phase for specific tasks and', 'start': 283.5, 'duration': 4.62}, {'text': 'the training process of a large language', 'start': 286.62, 'duration': 4.079}, {'text': 'model involves several steps the first', 'start': 288.12, 'duration': 5.76}, {'text': 'is text pre-processing the textual data', 'start': 290.699, 'duration': 4.621}, {'text': 'is transformed into a numerical', 'start': 293.88, 'duration': 3.36}, {'text': 'representation that can be effectively', 'start': 295.32, 'duration': 4.26}, {'text': 'processed by the model this conversion', 'start': 297.24, 'duration': 4.32}, {'text': 'may involve techniques like tokenization', 'start': 299.58, 'duration': 4.619}, {'text': 'encoding and creating input sequences', 'start': 301.56, 'duration': 4.98}, {'text': 'the next we have is random parameter', 'start': 304.199, 'duration': 5.041}, {'text': "initialization the model's parameters", 'start': 306.54, 'duration': 4.92}, {'text': 'are initialized randomly before the', 'start': 309.24, 'duration': 4.8}, {'text': 'training process begins the next is', 'start': 311.46, 'duration': 5.34}, {'text': 'inputting numerical data the numerical', 'start': 314.04, 'duration': 5.04}, {'text': 'representation of Text data is fed into', 'start': 316.8, 'duration': 4.86}, {'text': "the model for processing the model's", 'start': 319.08, 'duration': 4.14}, {'text': 'architecture typically based on', 'start': 321.66, 'duration': 3.9}, {'text': 'Transformers allows it to capture the', 'start': 323.22, 'duration': 4.02}, {'text': 'contextual relationships between the', 'start': 325.56, 'duration': 4.5}, {'text': 'words and tokens in the text and the', 'start': 327.24, 'duration': 4.799}, {'text': "next is Lowe's function calculation a", 'start': 330.06, 'duration': 3.96}, {'text': 'loss function is employed to measure the', 'start': 332.039, 'duration': 3.541}, {'text': "discrepancy between the model's", 'start': 334.02, 'duration': 3.66}, {'text': 'predictions and the actual next word or', 'start': 335.58, 'duration': 4.619}, {'text': 'token in a sentence the model aims to', 'start': 337.68, 'duration': 4.98}, {'text': 'minimize this loss during training and', 'start': 340.199, 'duration': 5.041}, {'text': 'the next is parameter optimization the', 'start': 342.66, 'duration': 4.44}, {'text': "model's parameters are adjusted through", 'start': 345.24, 'duration': 4.019}, {'text': 'optimization techniques such as gradient', 'start': 347.1, 'duration': 4.98}, {'text': 'descent to reduce the loss this involves', 'start': 349.259, 'duration': 4.741}, {'text': 'calculating radians and updating the', 'start': 352.08, 'duration': 3.78}, {'text': 'parameters accordingly gradually', 'start': 354.0, 'duration': 3.96}, {'text': "improving the model's performance and", 'start': 355.86, 'duration': 4.619}, {'text': 'the next is iterative training the', 'start': 357.96, 'duration': 4.019}, {'text': 'training process is repeated over', 'start': 360.479, 'duration': 4.081}, {'text': 'multiple iterations or epochs until the', 'start': 361.979, 'duration': 4.56}, {'text': "model's outputs achieve a satisfactory", 'start': 364.56, 'duration': 4.5}, {'text': 'level of accuracy on the given task or', 'start': 366.539, 'duration': 4.921}, {'text': 'data set by following this training', 'start': 369.06, 'duration': 4.68}, {'text': 'process large language model learn to', 'start': 371.46, 'duration': 4.14}, {'text': 'capture linguistic patterns understand', 'start': 373.74, 'duration': 4.56}, {'text': 'context and generate coherent responses', 'start': 375.6, 'duration': 5.099}, {'text': 'enabling them to excel at a wide range', 'start': 378.3, 'duration': 5.16}, {'text': 'of language related tasks and now we', 'start': 380.699, 'duration': 4.741}, {'text': 'will see how do large language models', 'start': 383.46, 'duration': 4.38}, {'text': 'work so large language models leverage', 'start': 385.44, 'duration': 4.68}, {'text': 'deep neural networks to generate outputs', 'start': 387.84, 'duration': 3.96}, {'text': 'based on patterns learned from the', 'start': 390.12, 'duration': 3.0}, {'text': 'training data', 'start': 391.8, 'duration': 4.26}, {'text': 'typically a large language model adopts', 'start': 393.12, 'duration': 5.1}, {'text': 'a Transformer architecture which enables', 'start': 396.06, 'duration': 4.079}, {'text': 'the model to identify relationships', 'start': 398.22, 'duration': 4.38}, {'text': 'between words in a sentence irrespective', 'start': 400.139, 'duration': 4.921}, {'text': 'of the position in the sequence in', 'start': 402.6, 'duration': 4.14}, {'text': 'contrast to referent neural networks', 'start': 405.06, 'duration': 4.68}, {'text': 'that is rnns that rely on recurrence to', 'start': 406.74, 'duration': 5.34}, {'text': 'capture token relationships Transformer', 'start': 409.74, 'duration': 4.679}, {'text': 'neural networks employ self-attention as', 'start': 412.08, 'duration': 4.08}, {'text': 'their primary mechanism', 'start': 414.419, 'duration': 3.541}, {'text': 'self-attention calculates attention', 'start': 416.16, 'duration': 4.2}, {'text': 'scores that determines the importance of', 'start': 417.96, 'duration': 4.799}, {'text': 'each token with respect to other tokens', 'start': 420.36, 'duration': 5.1}, {'text': 'in the text sequence facilitating the', 'start': 422.759, 'duration': 4.5}, {'text': 'modeling of intricate relationships', 'start': 425.46, 'duration': 3.299}, {'text': "within the data now we'll see", 'start': 427.259, 'duration': 3.421}, {'text': 'applications of large language models', 'start': 428.759, 'duration': 4.201}, {'text': 'last language models have wide range of', 'start': 430.68, 'duration': 4.5}, {'text': 'applications across various domains and', 'start': 432.96, 'duration': 4.5}, {'text': 'here are some notable applications the', 'start': 435.18, 'duration': 4.5}, {'text': 'first one is natural language processing', 'start': 437.46, 'duration': 4.5}, {'text': 'large language models are used to', 'start': 439.68, 'duration': 4.26}, {'text': 'improve natural language understanding', 'start': 441.96, 'duration': 4.98}, {'text': 'tasks such as sentiment analysis named', 'start': 443.94, 'duration': 5.4}, {'text': 'entity recognition text classification', 'start': 446.94, 'duration': 5.4}, {'text': 'and language modeling the next is chat', 'start': 449.34, 'duration': 5.16}, {'text': 'boards and virtual assistants large', 'start': 452.34, 'duration': 3.84}, {'text': 'language models power conversational', 'start': 454.5, 'duration': 4.139}, {'text': 'agents chatbots and virtual assistants', 'start': 456.18, 'duration': 4.32}, {'text': 'providing more interactive and', 'start': 458.639, 'duration': 4.081}, {'text': 'human-like interactions with users and', 'start': 460.5, 'duration': 5.039}, {'text': 'the next is machine translation large', 'start': 462.72, 'duration': 4.44}, {'text': 'language models have been used for', 'start': 465.539, 'duration': 3.78}, {'text': 'automatic language translation enabling', 'start': 467.16, 'duration': 3.659}, {'text': 'the translation of text between', 'start': 469.319, 'duration': 3.541}, {'text': 'different languages with improved', 'start': 470.819, 'duration': 4.081}, {'text': 'accuracy and the next we have is', 'start': 472.86, 'duration': 4.739}, {'text': 'sentiment analysis large language models', 'start': 474.9, 'duration': 4.5}, {'text': 'can analyze and classify as a sentiment', 'start': 477.599, 'duration': 4.021}, {'text': 'or emotion expressed in a piece of text', 'start': 479.4, 'duration': 4.139}, {'text': 'which is valuable for market research', 'start': 481.62, 'duration': 4.38}, {'text': 'brand monitoring and social media', 'start': 483.539, 'duration': 4.921}, {'text': 'analysis and the next we have is content', 'start': 486.0, 'duration': 4.62}, {'text': 'recommendation these models can be', 'start': 488.46, 'duration': 4.139}, {'text': 'employed to provide personalized content', 'start': 490.62, 'duration': 4.139}, {'text': 'recommendations enhancing user', 'start': 492.599, 'duration': 4.5}, {'text': 'experience and engagement on platforms', 'start': 494.759, 'duration': 4.44}, {'text': 'such as news websites or streaming', 'start': 497.099, 'duration': 3.961}, {'text': 'services these applications highlight', 'start': 499.199, 'duration': 4.381}, {'text': 'the versatility and potential impact of', 'start': 501.06, 'duration': 4.68}, {'text': 'large language models in various domains', 'start': 503.58, 'duration': 4.019}, {'text': 'improving language understanding', 'start': 505.74, 'duration': 3.899}, {'text': 'Automation and interaction between', 'start': 507.599, 'duration': 4.62}, {'text': 'humans and computers and with that we', 'start': 509.639, 'duration': 4.381}, {'text': 'have reached the end of this tutorial if', 'start': 512.219, 'duration': 3.421}, {'text': 'you have any questions please feel free', 'start': 514.02, 'duration': 3.6}, {'text': 'to comment and will have it answered for', 'start': 515.64, 'duration': 4.259}, {'text': 'you as soon as possible until next time', 'start': 517.62, 'duration': 4.44}, {'text': 'thank you for watching stay safe keep', 'start': 519.899, 'duration': 5.281}, {'text': 'learning and get ahead staying ahead in', 'start': 522.06, 'duration': 5.58}, {'text': 'your career requires continuous learning', 'start': 525.18, 'duration': 4.86}, {'text': "and upskilling whether you're a student", 'start': 527.64, 'duration': 5.819}, {'text': "aiming to learn today's top skills or a", 'start': 530.04, 'duration': 5.34}, {'text': 'working professional looking to advance', 'start': 533.459, 'duration': 4.641}, {'text': "your career we've got you covered", 'start': 535.38, 'duration': 5.579}, {'text': 'explore our impressive catalog of', 'start': 538.1, 'duration': 5.02}, {'text': 'certification programs in Cutting Edge', 'start': 540.959, 'duration': 5.041}, {'text': 'domains including data science cloud', 'start': 543.12, 'duration': 6.18}, {'text': 'computing cyber security AI machine', 'start': 546.0, 'duration': 6.48}, {'text': 'learning or digital marketing designed', 'start': 549.3, 'duration': 4.979}, {'text': 'in collaboration with leading', 'start': 552.48, 'duration': 4.68}, {'text': 'universities and top corporations and', 'start': 554.279, 'duration': 5.701}, {'text': 'delivered by industry experts choose any', 'start': 557.16, 'duration': 5.64}, {'text': 'of our programs and set yourself on the', 'start': 559.98, 'duration': 6.0}, {'text': 'path to Career Success click the link in', 'start': 562.8, 'duration': 6.44}, {'text': 'the description to know more', 'start': 565.98, 'duration': 3.26}, {'text': 'hi there if you like this video', 'start': 572.459, 'duration': 3.481}, {'text': 'subscribe to the simply learned YouTube', 'start': 574.14, 'duration': 4.56}, {'text': 'channel and click here to watch similar', 'start': 575.94, 'duration': 4.62}, {'text': 'videos turn it up and get certified', 'start': 578.7, 'duration': 4.02}, {'text': 'click here', 'start': 580.56, 'duration': 4.339}, {'text': 'foreign', 'start': 582.72, 'duration': 2.179}]
[{'title': 'What is an LLM (Large Language Model)?', 'description': 'Deep learning and large pools of data come together to form large language models, an AI-based algorithm. An LLM can ...', 'video_id': 'zKndCikg3R0', 'transcript': "humans need language to communicate soit makes sense that AI does too[Music]a large language model or llm is a typeof AI algorithm based on deep learningand huge amounts of data that canunderstand generate and predict newcontent language models aren't new thefirst AI language model can be tracedback to 1966 but large language modelsuse a significantly larger pool of datafor training which means a significantincrease in the capabilities of the AImodel one of the most commonapplications of llms right now isgenerating content using AI chat Botsmore and more are popping up in themarket as competitors look todifferentiate themselves check out thelink above or in the description belowto see how two of The Front Runners chatGPT and Bard compare and remember tosubscribe to ion tech for more videos onall things business Techso just how large are large languagemodels well there's no universallyaccepted figure for how large an llmtraining data set is but it's typicallyin the petabytes range for context asingle petabyte is equivalent to 1million gigabytes the human brain isbelieved to store about two and a halfpetabytes of memory datathe llm training consists of multiplesteps usually starting with unsupervisedlearning where the model starts toderive relationships between words andConcepts then fine-tuned with supervisedlearning the training data then passesthrough a Transformer which enables thellm to recognize relationships andconnections using a self-attentionmechanismonce the llm is trained it can serve asthe base for any AI usesllms can generate text translatelanguages summarize or rewrite contentorganize content analyze sentiment ofcontent like humor or tone and Conversenaturally with a user unlike oldergenerations of AI chatbot Technologiesllms can be particularly useful as afoundation for customized uses for bothbusinesses and individuals they're fastaccurate flexible and easy to trainhowever users should heed caution toollms come with a number of challengeslike the cost of deployment andoperation bias depending on what data itwas trained on AI hallucinations where aresponse is not based off of thetraining data troubleshooting complexityin glitch tokens or words or inputsmaliciously designed to make the llmmalfunction how have you used llms whatbenefits or challenges have youexperienced share your thoughts in thecomments and remember to like Andsubscribe[Music]foreign"}, {'title': 'LLM Explained |  What is LLM', 'description': 'Simple and easy explanation of LLM or Large Language Model in less than 5 minutes. In this short video, you will build an ...', 'video_id': '67_aMPDk2zw', 'transcript': "foreign[Music]has a curious parrot called buddy buddyhas a great mimicking ability and asharp memorybuddy listens to all the conversationsin Peter's home and can mimic them veryaccurately now when he hears feelinghungry I would like to have somefor this case the probability of himsaying Biryani cherries or food is muchhigher than the words such as bicycle orbookbut he doesn't understand the meaning ofBiryani or food or cherries the wayhumans do all he is doing is usingstatistical probability along with someRandomness to predict the next word orset of words be only based on the pastconversations he has listened to we cancall Buddy a stochastic parrotstochastic Means A system that ischaracterized by Randomness orprobabilitya language model is somewhat like astochastic parrot their computerprograms that use a technology calledneural networks to predict the next setof words for a sentence for a simpleexplanation of a neural network pleasewatch this particular videojust like how birdies strain on Peter'shome conversations data set you can havea language model that is trained on forexample all movie related articles fromWikipedia and it will be able to predictthe next set of words for a movierelated sentence Gmail autocomplete isone of the many applications that uses alanguage model underneathnow that we have some understanding of alanguage model let's understand what theheck is a large language modellet's go back to our buddy example ourbuddy got some Divine super power andnow he can listen to Peter's neighborsconversations conversations that arehappening in schools and universities inthe town in fact not only in his townbut all the towns across the worldwith this extra power and knowledge nowbuddy can complete the next set of wordson a history subjectgive your nutrition advice or even writea poem like our powerful parrot bodylarge language models are trained on ahuge volume of data such as Wikipediaarticles Google news articles onlinebooks and so onif you look inside the llm you will finda neural network containing trillions ofparameters that can capture more complexpatterns and nuances in a language chatGPT is an application that uses llmcalled gpt3 or gpt4 behind the scenesexamples of llms are Palm 2 by Googleand llama by metaon top of statistical predictions llmuses another approach calledreinforcement learning with humanfeedback rlhf let's understand this onceagain with Buddy one day Peter washaving a conversation with his cutelittle two-year-old sondon't eat too much bananas elsehearing this Peter realized that buddyhas been listening to the conversationsfrom abusive parents in his town what hesaid was the effect of thatPeter then starts skipping a close eyeon what buddy is saying for a samequestion buddy can produce multipleanswers and all Peter has to do is tellhim which one is toxic and which one isnotafter this training buddy doesn't useany toxic languagewhile training chat GPT open air used asimilar approach of human interventionrlhfopen air used a huge Workforce of humansto make chat GPT less toxic while llmsare very powerful they don't have anysubjective experience emotions orConsciousness that we as humans havellms work purely based on the data thatthey have been trained on I hope youlike this short explanation which wasbased on analogy obviously the technicalworking of this thing is littledifferent than analogy but this shouldgive you a good intuition on this topicif you like this video please share withthose who are curious about this topicforeign"}, {'title': 'What are Large Language Models (LLMs)?', 'description': 'Learn about Large Language Models (LLMs), a powerful neural network that enables computers to process and generate ...', 'video_id': 'iR2O2GPbB0E', 'transcript': 'SPEAKER 1: Unless you\'ve\nbeen living under a rock,you\'ve probably heard\nthat AI is gettingvery good at conversation.In fact, maybe you\neven chatted with oneof these AI\'s through a chat\nbot interface like Google Bard.SPEAKER 2: This is all\nthanks to a powerful kindof neural network called a\nLarge Language Model, or LLM.LLMs enable computers to\nunderstand and generatelanguage better\nthan ever before,unlocking a whole host\nof new applications.SPEAKER 1: In this\nvideo, we\'re goingto talk about what LLMs\nare and how anyone can getstarted building with them,\nwhether you\'re a developeror not.SPEAKER 2: Ready?BOTH: Let\'s dive in.SPEAKER 1: LLMs are\nmachine learning modelsthat are really good at\nunderstanding and generatinghuman language.They\'re based on transformers,\na type of neural networkarchitecture invented by Google.Now, what made the transformer\narchitecture so powerfulwas its ability to\nscale effectively,allowing us to train these\nmodels on massive textdatasets.SPEAKER 2: That\'s where the\n"large" in large languagemodels comes from--both the size and complexity\nof the neural network itself,as well as the size of the\ndataset that it was trained on.For some of these\nmodels, we\'re talkingabout trillions of\ntokens from a bunchof publicly available sources.And it wasn\'t until\nresearchers startedto make these\nmodels really largeand train them on\nthese huge datasetsthat they started showing\nthese impressive results,like understanding complex,\nnuanced language and generatinglanguage more\neloquently than ever.SPEAKER 1: If you\'re already\nfamiliar with machine learning,you probably think\nabout traininga model for a\nspecific task, like isthis tweet positive or\nnegative, or translate this textfrom French to English.What makes LLMs\nespecially powerfulis that one model can be used\nfor a whole variety of tasks,like chat, copywriting,\ntranslation, summarization,brainstorming, code generation,\nand a whole lot more.SPEAKER 2: Best of all, you can\nprototype language applicationsincredibly fast with LLMs--in just minutes,\nrather than months.And you don\'t have to be\na machine learning expertto do it.All you really need to\nknow is how to write.So how do you\nactually use an LLM?Well, let\'s take a look.LLMs learn about\npatterns and languagefrom the massive amounts of\ntext data they\'re trained on.Then they take as\ninput some textand produce some output text\nthat\'s likely to follow.SPEAKER 1: Another\nway to say thisis that LLMs are like really\nsophisticated autocomplete.So for example, if we\ngive an LLM the input--SPEAKER 2: It\'s\nraining cats and--SPEAKER 1: It\'ll probably\npredict that "dogs" isthe most likely word to follow.Now, this might not\nseem that exciting,but we can actually use this\nautocomplete-like functionalityto solve tons of tasks just by\nwriting strategic text input.SPEAKER 2: For example,\nlet\'s take Google\'s PaLMLLM and input this sentence.SPEAKER 1: I have two\napples and I eat one.I\'m left with--SPEAKER 2: The PaLM model\noutputs the answer "one."In this way, we get the LLM\nto perform some simple math.SPEAKER 1: Or take\nanother example.SPEAKER 2: Paris is to\nFrance as Tokyo is to--SPEAKER 1: The PaLM model\noutputs "Japan," which tells usthat the model can not\nonly complete analogies,but it also has some\nworld knowledge that it\'slearned from its training data.So I should add the\ncaveat that not allof the knowledge\nthat the LLM outputsis necessarily\nfactually accurate.SPEAKER 2: Now, all of the\ntext that we feed into an LLMas input is called a\nprompt, and it turns outthere\'s this whole art known\nas prompt design, whichis about figuring out\nhow to write and formatprompt text to get LLMs\nto do what you want.SPEAKER 1: For example,\none way to structurea prompt is as an\ninstruction, like--SPEAKER 2: Write me a\npoem about Ada Lovelacein the style of Shakespeare.SPEAKER 1: Or explain quantum\nphysics to me like I\'m five.SPEAKER 2: Or generate\na list of itemsI need for a camping trip\nto Yosemite National Park.SPEAKER 1: This approach--\nusing a single commandto get an LLM to\ntake on a behavior--is called zero shot learning.But in addition to just\nproviding an instruction,it can be helpful to\nshow the model whatyou want by adding examples.This is called few shot\nlearning because we showedthe model a few examples.Like here\'s a prompt for\ntranslating from Englishto French.First we provide an instruction.Then we give some examples,\nestablishing the text pattern.If we pass this prompt\nto an LLM like PaLM,we get back something\nlike the following.SPEAKER 2: The model did\nprovide a French translationof lipstick, but you might\nnotice that it went onto generate all these additional\nEnglish-French translationpairs.This might seem a\nlittle unexpected,but the LLM is just\ncompleting the patternthat we gave it in the prompt.As another example,\nhere\'s a few shotprompt to convert Python\ncode snippets to JavaScript.Our prompt starts\nwith an instruction,then we have some examples,\nand finally, the Python codewe actually want converted.The very last part\nof this promptis JavaScript colon\nbecause we wantto nudge the model to output\nsome JavaScript code justlike this.SPEAKER 1: Note that\nin a real application,we probably want to parameterize\nthe input instead of hardcoding it into the prompt.That way, our users can\nprovide the Python codethat they want converted.And this is essentially how\nyou would customize an LLMfor a Python to JavaScript app.SPEAKER 2: Now, you\nmight be wonderingwhat the absolute best way\nto write a model prompt is.And if so, we\'ve got\nsome bad news for you.SPEAKER 1: There\'s\ncurrently no optimal wayto write model\nprompts, and that\'sbecause the results we get\nare so highly dependenton the underlying model.Sometimes small changes in\nwording or even in word ordercan improve the LLM\'s outputs\nin ways that are not alwayspredictable.SPEAKER 2: That\'s\nwhy it\'s always worthtrying out lots of different\nstructures and examplesand formats and seeing what\nworks best for your use case.SPEAKER 1: There you have it.That\'s the magic of\nLLMs in a nutshell.SPEAKER 2: You can check\nout Bard at bard.google.com,and definitely let us\nknow in the commentsbelow what you\'re\nbuilding with LLMs.[MUSIC PLAYING]'}, {'title': 'How Large Language Models Work', 'description': 'Learn about watsonx → https://ibm.biz/BdvxRj Large language models-- or LLMs --are a type of generative pretrained transformer ...', 'video_id': '5sLYAQS9sWQ', 'transcript': ''}, {'title': 'Large Language Models Explained | What Is Large Language Model (LLM) | Machine Learning |Simplilearn', 'description': 'Purdue Post Graduate Program In AI And Machine Learning: ...', 'video_id': 'ou7IswWBZbY', 'transcript': "hello everyone and welcome to the videoon large language models large languagemodels are basically very Advancedartificial intelligence systems that canprocess and generate massive amounts oftext Data they are designed to learnfrom and understand natural humanlanguage and can be used to perform awide range of language related taskssuch as translation speech recognitionand automatic summary generation one ofthe key advantages of large languagemodels is their ability to learn fromvast amounts of data which allows themto generate highly accurate andrealistic responses to complex naturallanguage prompts additionally they canbe trained on multiple languagessimultaneously which means they can beused to perform language translationbetween different pairs of languagesmaking them an invaluable tool forbusinesses and organizations that workwith people from diverse culturalbackgrounds not just language relatedtasks but llms can also have thepotential to revolutionize Fields suchas research science and Healthcare byallowing researchers to quickly analyzeand process vast amounts of complex Xdata they can accelerate progress inareas such as drug Discovery HealthcareDiagnostics and AI development thatbeing said large language models alsopresent some challenges for instancethey require a significant amount ofcompute resources which can beprohibitively expensive for someorganizations additionally because theyare trained on huge amounts of data theymay inadvertently reinforce biased ordiscriminate the patterns in languageuse which is something that researchersand Engineers are continuously workingto address overall large language modelsare an exciting and rapidly developingfield with tremendous potential totransform the way we live work andcommunicate so if you want to embark ona journey of AI and ml then try giving ashow to a postgraduate program in Ai andml that is in partnership with IBM thisartificial intelligence course coversthe latest tools and Technologies fromthe AI ecosystem and features masterclasses by Caltech faculty and IBMexperts hackathons and ask me anythingsessions this program showcases Caltechctm is excellence and I IBM's industryprogress the artificial intelligencecourse covers key Concepts likestatistics data science with pythonmachine learning deep learning NLP andreinforcement learning through anInteractive Learning model with livesessions enroll now analog exciting Aiand ml opportunities the link ismentioned in the description box belowand with that having said hey everyonewelcome to Simply learns YouTube channelbut before we dive into that don'tforget to like subscribe and share inthis video we'll cover topics like whatare large language models after thatwe'll look at what large language modelsused for and after that we'll cover howare large language models trained afterthis we'll look at how do large languagemodels work and at the last we'll seesome applications of large languagemodels so without any further Ado let'sget startedso we'll start with what are the largelanguage models large language modelssuch as gpd3 generative pre-trainedTransformer 3 Advanced artificialintelligence systems designed tounderstand and generate human-like textthese models are built using deeplearning techniques and have beentrained on vast amounts of text Datafrom the internet these models useself-attention mechanisms to analyze therelationships between words or tokens ina text enabling them to capturecontextual information and generate coreentry responses these models useself-attention mechanisms to analyze therelationship between different words ortokens in the text enabling them tocapture contextual information andgenerate core and responses these modelshave significant implications for a widerange of applications including virtualassistants chat boards contentgenerationlanguage translation and aiding inresearch and decision making processestheir ability to generate coherent andcontextually appropriate text has led toadvancement in natural languageunderstanding and human computerinteractionnow we'll see what are language modelsused forso large language models are utilized inscenarios where there is limited or nodomain specific data available fortraining these scenarios include bothfew short and zero short learningapproaches which rely on the model'sstrong inductive bias and its capabilityto derive meaningful representationsfrom a small amount of data or even nodata at allnow we'll see how our large languagemodels trained large language modelstypically undergo pre-training on abroadwhile encompassing data set that sharesstatistical similarities with the dataset specific to the Target task theobjective of pre-training is to enablethe model to acquire high level featuresthat can later be applied during thefine tuning phase for specific tasks andthe training process of a large languagemodel involves several steps the firstis text pre-processing the textual datais transformed into a numericalrepresentation that can be effectivelyprocessed by the model this conversionmay involve techniques like tokenizationencoding and creating input sequencesthe next we have is random parameterinitialization the model's parametersare initialized randomly before thetraining process begins the next isinputting numerical data the numericalrepresentation of Text data is fed intothe model for processing the model'sarchitecture typically based onTransformers allows it to capture thecontextual relationships between thewords and tokens in the text and thenext is Lowe's function calculation aloss function is employed to measure thediscrepancy between the model'spredictions and the actual next word ortoken in a sentence the model aims tominimize this loss during training andthe next is parameter optimization themodel's parameters are adjusted throughoptimization techniques such as gradientdescent to reduce the loss this involvescalculating radians and updating theparameters accordingly graduallyimproving the model's performance andthe next is iterative training thetraining process is repeated overmultiple iterations or epochs until themodel's outputs achieve a satisfactorylevel of accuracy on the given task ordata set by following this trainingprocess large language model learn tocapture linguistic patterns understandcontext and generate coherent responsesenabling them to excel at a wide rangeof language related tasks and now wewill see how do large language modelswork so large language models leveragedeep neural networks to generate outputsbased on patterns learned from thetraining datatypically a large language model adoptsa Transformer architecture which enablesthe model to identify relationshipsbetween words in a sentence irrespectiveof the position in the sequence incontrast to referent neural networksthat is rnns that rely on recurrence tocapture token relationships Transformerneural networks employ self-attention astheir primary mechanismself-attention calculates attentionscores that determines the importance ofeach token with respect to other tokensin the text sequence facilitating themodeling of intricate relationshipswithin the data now we'll seeapplications of large language modelslast language models have wide range ofapplications across various domains andhere are some notable applications thefirst one is natural language processinglarge language models are used toimprove natural language understandingtasks such as sentiment analysis namedentity recognition text classificationand language modeling the next is chatboards and virtual assistants largelanguage models power conversationalagents chatbots and virtual assistantsproviding more interactive andhuman-like interactions with users andthe next is machine translation largelanguage models have been used forautomatic language translation enablingthe translation of text betweendifferent languages with improvedaccuracy and the next we have issentiment analysis large language modelscan analyze and classify as a sentimentor emotion expressed in a piece of textwhich is valuable for market researchbrand monitoring and social mediaanalysis and the next we have is contentrecommendation these models can beemployed to provide personalized contentrecommendations enhancing userexperience and engagement on platformssuch as news websites or streamingservices these applications highlightthe versatility and potential impact oflarge language models in various domainsimproving language understandingAutomation and interaction betweenhumans and computers and with that wehave reached the end of this tutorial ifyou have any questions please feel freeto comment and will have it answered foryou as soon as possible until next timethank you for watching stay safe keeplearning and get ahead staying ahead inyour career requires continuous learningand upskilling whether you're a studentaiming to learn today's top skills or aworking professional looking to advanceyour career we've got you coveredexplore our impressive catalog ofcertification programs in Cutting Edgedomains including data science cloudcomputing cyber security AI machinelearning or digital marketing designedin collaboration with leadinguniversities and top corporations anddelivered by industry experts choose anyof our programs and set yourself on thepath to Career Success click the link inthe description to know morehi there if you like this videosubscribe to the simply learned YouTubechannel and click here to watch similarvideos turn it up and get certifiedclick hereforeign"}]
